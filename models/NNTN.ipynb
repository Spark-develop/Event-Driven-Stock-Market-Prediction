{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bit8717ee642559407d8dab54eb44bff6f2",
   "display_name": "Python 3.7.9 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "684b1123683431d89d3bfe9a89cc763215f4b8cd94b4aba1fb40ad45ff7c8b41"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import math\r\n",
    "import pickle\r\n",
    "import numpy as np\r\n",
    "from numpy.core.numeric import indices\r\n",
    "import scipy.optimize\r\n",
    "import scipy.sparse as sp\r\n",
    "from tensorflow.python.keras.utils.data_utils import Sequence"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class NeuralTensorNetwork(object):\r\n",
    "    def __init__(self, program_parameters):\r\n",
    "\r\n",
    "        self.num_words           = program_parameters['num_words']\r\n",
    "        self.embedding_size      = program_parameters['embedding_size']\r\n",
    "        self.num_entities        = program_parameters['num_entities']\r\n",
    "        self.num_relations       = program_parameters['num_relations']\r\n",
    "        self.batch_size          = program_parameters['batch_size']\r\n",
    "        self.slice_size          = program_parameters['slice_size']\r\n",
    "        self.word_indices        = program_parameters['word_indices']\r\n",
    "        self.activation_function = program_parameters['activation_function']\r\n",
    "        self.lamda               = program_parameters['lamda']\r\n",
    "\r\n",
    "        r = 0.0001\r\n",
    "        word_vectors = np.random.random((self.embedding_size, self.num_words)) * 2 * r - r\r\n",
    "        r = 1 / math.sqrt(2 * self.embedding_size)\r\n",
    "\r\n",
    "        W = {}\r\n",
    "        V = {}\r\n",
    "        b = {} \r\n",
    "        U = {}\r\n",
    "\r\n",
    "        for i in range(self.num_relations):\r\n",
    "\r\n",
    "\r\n",
    "            W[i] = np.random.random((self.embedding_size, self.embedding_size, self.slice_size)) * 2 * r - r\r\n",
    "            V[i] = np.zeros((2 * self.embedding_size, self.slice_size))\r\n",
    "            b[i] = np.zeros((1, self.slice_size))\r\n",
    "            U[i] = np.ones((self.slice_size, 1))\r\n",
    "\r\n",
    "\r\n",
    "        self.theta, self.decode_info = self.S2P(W, V, b, U, word_vectors)\r\n",
    "\r\n",
    "\r\n",
    "    def S2P(self, W,V,b,U,word_vectors):\r\n",
    "        theta       = []\r\n",
    "        decode_info = {}\r\n",
    "        arguments = [W,V,b,U,word_vectors]\r\n",
    "        # print(word_vectors)\r\n",
    "        for i in range(len(arguments)):\r\n",
    "            argument = arguments[i]\r\n",
    "            print(i)\r\n",
    "            if isinstance(argument, dict):\r\n",
    "                print('if')\r\n",
    "                decode_cell = {}\r\n",
    "                for j in range(len(argument)):\r\n",
    "                    decode_cell[j] = argument[j].shape\r\n",
    "                    theta          = np.concatenate((theta, argument[j].flatten()))\r\n",
    "                decode_info[i] = decode_cell\r\n",
    "            else:\r\n",
    "                print('else')\r\n",
    "                decode_info[i] = word_vectors.shape\r\n",
    "                theta          = np.concatenate((theta, argument.flatten()))\r\n",
    "\r\n",
    "        return theta, decode_info\r\n",
    "\r\n",
    "\r\n",
    "    def P2S(self, theta):\r\n",
    "\r\n",
    "\r\n",
    "        stack = []\r\n",
    "        index = 0\r\n",
    "\r\n",
    "        for i in range(len(self.decode_info)):\r\n",
    "\r\n",
    "\r\n",
    "            decode_cell = self.decode_info[i]\r\n",
    "\r\n",
    "            if isinstance(decode_cell, dict):\r\n",
    "\r\n",
    "\r\n",
    "                param_dict = {}\r\n",
    "\r\n",
    "                for j in range(len(decode_cell)):\r\n",
    "\r\n",
    "\r\n",
    "                    param_dict[j] = theta[index : index + np.prod(decode_cell[j])].reshape(decode_cell[j])\r\n",
    "                    index        += np.prod(decode_cell[j])\r\n",
    "\r\n",
    "                stack.append(param_dict)\r\n",
    "\r\n",
    "            else:\r\n",
    "                stack.append(theta[index : index + np.prod(decode_cell)].reshape(decode_cell))\r\n",
    "                index += np.prod(decode_cell)\r\n",
    "\r\n",
    "        return stack\r\n",
    "\r\n",
    "\r\n",
    "    def act_F(self, x):\r\n",
    "\r\n",
    "        if self.activation_function == 0:\r\n",
    "\r\n",
    "\r\n",
    "            return np.tanh(x)\r\n",
    "\r\n",
    "        elif self.activation_function == 1:\r\n",
    "\r\n",
    "\r\n",
    "            return (1 / (1 + np.exp(-x)))\r\n",
    "\r\n",
    "\r\n",
    "    def ActDifferential(self, x):\r\n",
    "\r\n",
    "        if self.activation_function == 0:\r\n",
    "\r\n",
    "\r\n",
    "            return (1 - np.power(x, 2))\r\n",
    "\r\n",
    "        elif self.activation_function == 1:\r\n",
    "\r\n",
    "\r\n",
    "            return (x * (1 - x))\r\n",
    "\r\n",
    "\r\n",
    "    def costF(self, theta, data_batch, flip):\r\n",
    "\r\n",
    "\r\n",
    "        W, V, b, U, word_vectors = self.P2S(theta)\r\n",
    "\r\n",
    "\r\n",
    "        entity_vectors = np.zeros((self.embedding_size, self.num_entities))\r\n",
    "        entity_vector_grad = np.zeros((self.embedding_size, self.num_entities))\r\n",
    "\r\n",
    "\r\n",
    "        for entity in range(self.num_entities):\r\n",
    "\r\n",
    "            entity_vectors[:, entity] = np.mean(word_vectors[:, self.word_indices[entity]], axis = 1)\r\n",
    "\r\n",
    "\r\n",
    "        cost = 0\r\n",
    "\r\n",
    "\r\n",
    "        W_grad = {}; V_grad = {}; b_grad = {}; U_grad = {}\r\n",
    "\r\n",
    "        for i in range(self.num_relations):\r\n",
    "\r\n",
    "\r\n",
    "            rel_i_list = (data_batch['rel'] == i)\r\n",
    "            num_rel_i = np.sum(rel_i_list)\r\n",
    "\r\n",
    "\r\n",
    "            e1 = data_batch['e1'][rel_i_list]\r\n",
    "            e2 = data_batch['e2'][rel_i_list]\r\n",
    "            e3 = data_batch['e3'][rel_i_list]\r\n",
    "\r\n",
    "\r\n",
    "            entity_vectors_e1 = entity_vectors[:, e1.tolist()]\r\n",
    "            entity_vectors_e2 = entity_vectors[:, e2.tolist()]\r\n",
    "            entity_vectors_e3 = entity_vectors[:, e3.tolist()]\r\n",
    "\r\n",
    "\r\n",
    "            if flip:\r\n",
    "\r\n",
    "                entity_vectors_e1_neg = entity_vectors_e1\r\n",
    "                entity_vectors_e2_neg = entity_vectors_e3\r\n",
    "                e1_neg = e1\r\n",
    "                e2_neg = e3\r\n",
    "\r\n",
    "            else:\r\n",
    "\r\n",
    "                entity_vectors_e1_neg = entity_vectors_e3\r\n",
    "                entity_vectors_e2_neg = entity_vectors_e2\r\n",
    "                e1_neg = e3\r\n",
    "                e2_neg = e2\r\n",
    "\r\n",
    "\r\n",
    "            preactivation_pos = np.zeros((self.slice_size, num_rel_i))\r\n",
    "            preactivation_neg = np.zeros((self.slice_size, num_rel_i))\r\n",
    "\r\n",
    "\r\n",
    "            for slice in range(self.slice_size):\r\n",
    "\r\n",
    "                preactivation_pos[slice, :] = np.sum(entity_vectors_e1 *\r\n",
    "                    np.dot(W[i][:, :, slice], entity_vectors_e2), axis = 0)\r\n",
    "                preactivation_neg[slice, :] = np.sum(entity_vectors_e1_neg *\r\n",
    "                    np.dot(W[i][:, :, slice], entity_vectors_e2_neg), axis = 0)\r\n",
    "\r\n",
    "            preactivation_pos += b[i].T + np.dot(V[i].T, np.vstack((entity_vectors_e1, entity_vectors_e2)))\r\n",
    "            preactivation_neg += b[i].T + np.dot(V[i].T, np.vstack((entity_vectors_e1_neg, entity_vectors_e2_neg)))\r\n",
    "\r\n",
    "\r\n",
    "            activation_pos = self.act_F(preactivation_pos)\r\n",
    "            activation_neg = self.act_F(preactivation_neg)\r\n",
    "\r\n",
    "\r\n",
    "            score_pos = np.dot(U[i].T, activation_pos)\r\n",
    "            score_neg = np.dot(U[i].T, activation_neg)\r\n",
    "\r\n",
    "\r\n",
    "            wrong_filter = (score_pos + 1 > score_neg)[0]\r\n",
    "\r\n",
    "\r\n",
    "            cost += np.sum(wrong_filter * (score_pos - score_neg + 1)[0])\r\n",
    "\r\n",
    "\r\n",
    "            W_grad[i] = np.zeros(W[i].shape)\r\n",
    "            V_grad[i] = np.zeros(V[i].shape)\r\n",
    "\r\n",
    "\r\n",
    "            num_wrong = np.sum(wrong_filter)\r\n",
    "\r\n",
    "\r\n",
    "            activation_pos            = activation_pos[:, wrong_filter]\r\n",
    "            activation_neg            = activation_neg[:, wrong_filter]\r\n",
    "            entity_vectors_e1_rel     = entity_vectors_e1[:, wrong_filter]\r\n",
    "            entity_vectors_e2_rel     = entity_vectors_e2[:, wrong_filter]\r\n",
    "            entity_vectors_e1_rel_neg = entity_vectors_e1_neg[:, wrong_filter]\r\n",
    "            entity_vectors_e2_rel_neg = entity_vectors_e2_neg[:, wrong_filter]\r\n",
    "\r\n",
    "\r\n",
    "            e1     = e1[wrong_filter]\r\n",
    "            e2     = e2[wrong_filter]\r\n",
    "            e1_neg = e1_neg[wrong_filter]\r\n",
    "            e2_neg = e2_neg[wrong_filter]\r\n",
    "\r\n",
    "            U_grad[i] = np.sum(activation_pos - activation_neg, axis = 1).reshape(self.slice_size, 1)\r\n",
    "\r\n",
    "            temp_pos_all = U[i] * self.ActDifferential(activation_pos)\r\n",
    "            temp_neg_all = - U[i] * self.ActDifferential(activation_neg)\r\n",
    "\r\n",
    "            b_grad[i] = np.sum(temp_pos_all + temp_neg_all, axis = 1).reshape(1, self.slice_size)\r\n",
    "\r\n",
    "            values = np.ones(num_wrong)\r\n",
    "            rows   = np.arange(num_wrong + 1)\r\n",
    "\r\n",
    "            e1_sparse     = sp.csr_matrix((values, e1, rows), shape = (num_wrong, self.num_entities))\r\n",
    "            e2_sparse     = sp.csr_matrix((values, e2, rows), shape = (num_wrong, self.num_entities))\r\n",
    "            e1_neg_sparse = sp.csr_matrix((values, e1_neg, rows), shape = (num_wrong, self.num_entities))\r\n",
    "            e2_neg_sparse = sp.csr_matrix((values, e2_neg, rows), shape = (num_wrong, self.num_entities))\r\n",
    "\r\n",
    "            for k in range(self.slice_size):\r\n",
    "\r\n",
    "                temp_pos = temp_pos_all[k, :].reshape(1, num_wrong)\r\n",
    "                temp_neg = temp_neg_all[k, :].reshape(1, num_wrong)\r\n",
    "\r\n",
    "                W_grad[i][:, :, k] = np.dot(entity_vectors_e1_rel * temp_pos, entity_vectors_e2_rel.T) \\\r\n",
    "                    + np.dot(entity_vectors_e1_rel_neg * temp_neg, entity_vectors_e2_rel_neg.T)\r\n",
    "\r\n",
    "                V_grad[i][:, k] = np.sum(np.vstack((entity_vectors_e1_rel, entity_vectors_e2_rel)) * temp_pos\r\n",
    "                    + np.vstack((entity_vectors_e1_rel_neg, entity_vectors_e2_rel_neg)) * temp_neg, axis = 1)\r\n",
    "\r\n",
    "                V_pos = V[i][:, k].reshape(2*self.embedding_size, 1) * temp_pos\r\n",
    "                V_neg = V[i][:, k].reshape(2*self.embedding_size, 1) * temp_neg\r\n",
    "\r\n",
    "                entity_vector_grad += V_pos[:self.embedding_size, :] * e1_sparse + V_pos[self.embedding_size:, :] * e2_sparse \\\r\n",
    "                    + V_neg[:self.embedding_size, :] * e1_neg_sparse + V_neg[self.embedding_size:, :] * e2_neg_sparse\r\n",
    "\r\n",
    "                entity_vector_grad += (np.dot(W[i][:, :, k], entity_vectors[:, e2.tolist()]) * temp_pos) * e1_sparse \\\r\n",
    "                    + (np.dot(W[i][:, :, k].T, entity_vectors[:, e1.tolist()]) * temp_pos) * e2_sparse \\\r\n",
    "                    + (np.dot(W[i][:, :, k], entity_vectors[:, e2_neg.tolist()]) * temp_neg) * e1_neg_sparse \\\r\n",
    "                    + (np.dot(W[i][:, :, k].T, entity_vectors[:, e1_neg.tolist()]) * temp_neg) * e2_neg_sparse\r\n",
    "\r\n",
    "\r\n",
    "            W_grad[i] /= self.batch_size\r\n",
    "            V_grad[i] /= self.batch_size\r\n",
    "            b_grad[i] /= self.batch_size\r\n",
    "            U_grad[i] /= self.batch_size\r\n",
    "\r\n",
    "\r\n",
    "        word_vector_grad = np.zeros(word_vectors.shape)\r\n",
    "\r\n",
    "\r\n",
    "        for entity in range(self.num_entities):\r\n",
    "\r\n",
    "            entity_len = len(self.word_indices[entity])\r\n",
    "            word_vector_grad[:, self.word_indices[entity]] += \\\r\n",
    "                np.tile(entity_vector_grad[:, entity].reshape(self.embedding_size, 1) / entity_len, (1, entity_len))\r\n",
    "\r\n",
    "\r\n",
    "        word_vector_grad /= self.batch_size\r\n",
    "        cost             /= self.batch_size\r\n",
    "\r\n",
    "\r\n",
    "        theta_grad, d_t = self.S2P(W_grad, V_grad, b_grad, U_grad, word_vector_grad)\r\n",
    "\r\n",
    "\r\n",
    "        cost       += 0.5 * self.lamda * np.sum(theta * theta)\r\n",
    "        theta_grad += self.lamda * theta\r\n",
    "\r\n",
    "        return cost, theta_grad\r\n",
    "\r\n",
    "    def MaxThresholds(self, dev_data, dev_labels):\r\n",
    "\r\n",
    "\r\n",
    "        W, V, b, U, word_vectors = self.P2S(self.theta)\r\n",
    "\r\n",
    "\r\n",
    "        entity_vectors = np.zeros((self.embedding_size, self.num_entities))\r\n",
    "\r\n",
    "\r\n",
    "        for entity in range(self.num_entities):\r\n",
    "\r\n",
    "            entity_vectors[:, entity] = np.mean(word_vectors[:, self.word_indices[entity]], axis = 1)\r\n",
    "\r\n",
    "\r\n",
    "        dev_scores = np.zeros(dev_labels.shape)\r\n",
    "\r\n",
    "        for i in range(dev_data.shape[0]):\r\n",
    "\r\n",
    "            rel = dev_data[i, 1]\r\n",
    "            entity_vector_e1  = entity_vectors[:, dev_data[i, 0]].reshape(self.embedding_size, 1)\r\n",
    "            entity_vector_e2  = entity_vectors[:, dev_data[i, 2]].reshape(self.embedding_size, 1)\r\n",
    "\r\n",
    "            entity_stack = np.vstack((entity_vector_e1, entity_vector_e2))\r\n",
    "\r\n",
    "            for k in range(self.slice_size):\r\n",
    "\r\n",
    "                dev_scores[i, 0] += U[rel][k, 0] * \\\r\n",
    "                   (np.dot(entity_vector_e1.T, np.dot(W[rel][:, :, k], entity_vector_e2)) +\r\n",
    "                    np.dot(V[rel][:, k].T, entity_stack) + b[rel][0, k])\r\n",
    "\r\n",
    "        score_min = np.min(dev_scores)\r\n",
    "        score_max = np.max(dev_scores)\r\n",
    "\r\n",
    "        best_thresholds = np.empty((self.num_relations, 1))\r\n",
    "        best_accuracies = np.empty((self.num_relations, 1))\r\n",
    "\r\n",
    "        for i in range(self.num_relations):\r\n",
    "\r\n",
    "            best_thresholds[i, :] = score_min\r\n",
    "            best_accuracies[i, :] = -1\r\n",
    "\r\n",
    "        score_temp = score_min\r\n",
    "        interval   = 0.01\r\n",
    "\r\n",
    "        while(score_temp <= score_max):\r\n",
    "\r\n",
    "            for i in range(self.num_relations):\r\n",
    "\r\n",
    "                rel_i_list    = (dev_data[:, 1] == i)\r\n",
    "                predictions   = (dev_scores[rel_i_list, 0] <= score_temp) * 2 - 1\r\n",
    "                temp_accuracy = np.mean((predictions == dev_labels[rel_i_list, 0]))\r\n",
    "\r\n",
    "                if(temp_accuracy > best_accuracies[i, 0]):\r\n",
    "\r\n",
    "                    best_accuracies[i, 0] = temp_accuracy\r\n",
    "                    best_thresholds[i, 0] = score_temp\r\n",
    "\r\n",
    "            score_temp += interval\r\n",
    "\r\n",
    "        self.best_thresholds = best_thresholds\r\n",
    "\r\n",
    "    def getPrediction(self, test_data):\r\n",
    "\r\n",
    "        W, V, b, U, word_vectors = self.P2S(self.theta)\r\n",
    "\r\n",
    "        entity_vectors = np.zeros((self.embedding_size, self.num_entities))\r\n",
    "\r\n",
    "        for entity in range(self.num_entities):\r\n",
    "\r\n",
    "            entity_vectors[:, entity] = np.mean(word_vectors[:, self.word_indices[entity]], axis = 1)\r\n",
    "\r\n",
    "        predictions = np.empty((test_data.shape[0], 1))\r\n",
    "\r\n",
    "        for i in range(test_data.shape[0]):\r\n",
    "\r\n",
    "            rel = test_data[i, 1]\r\n",
    "            entity_vector_e1  = entity_vectors[:, test_data[i, 0]].reshape(self.embedding_size, 1)\r\n",
    "            entity_vector_e2  = entity_vectors[:, test_data[i, 2]].reshape(self.embedding_size, 1)\r\n",
    "\r\n",
    "            entity_stack = np.vstack((entity_vector_e1, entity_vector_e2))\r\n",
    "            test_score   = 0\r\n",
    "\r\n",
    "            for k in range(self.slice_size):\r\n",
    "\r\n",
    "                test_score += U[rel][k, 0] * \\\r\n",
    "                   (np.dot(entity_vector_e1.T, np.dot(W[rel][:, :, k], entity_vector_e2)) +\r\n",
    "                    np.dot(V[rel][:, k].T, entity_stack) + b[rel][0, k])\r\n",
    "\r\n",
    "            if(test_score <= self.best_thresholds[rel, 0]):\r\n",
    "                predictions[i, 0] = 1\r\n",
    "            else:\r\n",
    "                predictions[i, 0] = -1\r\n",
    "\r\n",
    "        return predictions\r\n",
    "\r\n",
    "def getTest(file_name, entity_dictionary, relation_dictionary):\r\n",
    "\r\n",
    "    file_object = open(file_name, 'r')\r\n",
    "    data        = file_object.read().splitlines()\r\n",
    "\r\n",
    "    num_entries = len(data)\r\n",
    "    test_data   = np.empty((num_entries, 3))\r\n",
    "    labels      = np.empty((num_entries, 1))\r\n",
    "\r\n",
    "    index = 0\r\n",
    "\r\n",
    "    for line in data:\r\n",
    "\r\n",
    "        entity1, relation, entity2, label = line.split()\r\n",
    "\r\n",
    "        test_data[index, 0] = entity_dictionary[entity1]\r\n",
    "        test_data[index, 1] = relation_dictionary[relation]\r\n",
    "        test_data[index, 2] = entity_dictionary[entity2]\r\n",
    "\r\n",
    "        if label == '1':\r\n",
    "            labels[index, 0] = 1\r\n",
    "        else:\r\n",
    "            labels[index, 0] = -1\r\n",
    "\r\n",
    "        index += 1\r\n",
    "\r\n",
    "    return test_data, labels\r\n",
    "\r\n",
    "def WordIndices(file_name):\r\n",
    "\r\n",
    "    word_dictionary = pickle.load(open(file_name, 'rb'))\r\n",
    "    num_words    = word_dictionary['num_words']\r\n",
    "    num_words = num_words[len(num_words)-1]\r\n",
    "    word_indices = word_dictionary['word_indices'][0:]\r\n",
    "    return word_indices, num_words\r\n",
    "\r\n",
    "def TrainingData(file_name, entity_dictionary, relation_dictionary):\r\n",
    "\r\n",
    "    file_object = open(file_name, 'r')\r\n",
    "    data        = file_object.read().splitlines()\r\n",
    "\r\n",
    "    num_examples  = len(data)\r\n",
    "    training_data = np.empty((num_examples, 3))\r\n",
    "\r\n",
    "    index = 0\r\n",
    "    for line in data:\r\n",
    "\r\n",
    "        entity1, relation, entity2 = line.split()\r\n",
    "\r\n",
    "        training_data[index, 0] = entity_dictionary[entity1]\r\n",
    "        training_data[index, 1] = relation_dictionary[relation]\r\n",
    "        training_data[index, 2] = entity_dictionary[entity2]\r\n",
    "\r\n",
    "        index += 1\r\n",
    "\r\n",
    "    return training_data, num_examples\r\n",
    "\r\n",
    "def Dictionary(file_name):\r\n",
    "\r\n",
    "    file_object = open(file_name, 'r')\r\n",
    "    data = file_object.read().splitlines()\r\n",
    "\r\n",
    "    dictionary = {}\r\n",
    "    index = 0\r\n",
    "\r\n",
    "    for entity in data:\r\n",
    "        dictionary[entity] = index\r\n",
    "        index += 1\r\n",
    "\r\n",
    "    num_entries = index\r\n",
    "\r\n",
    "    return dictionary, num_entries\r\n",
    "\r\n",
    "def ProgramParameters():\r\n",
    "\r\n",
    "    program_parameters = {}\r\n",
    "\r\n",
    "    program_parameters['embedding_size']      = 100  \r\n",
    "    program_parameters['slice_size']          = 3      \r\n",
    "    program_parameters['num_iterations']      = 500   \r\n",
    "    program_parameters['batch_size']          = 20000  \r\n",
    "    program_parameters['corrupt_size']        = 10\r\n",
    "    program_parameters['activation_function'] = 0      \r\n",
    "    program_parameters['lamda']               = 0.0001\r\n",
    "    program_parameters['batch_iterations']    = 5      \r\n",
    "\r\n",
    "    return program_parameters\r\n",
    "\r\n",
    "\r\n",
    "def NTN():\r\n",
    "\r\n",
    "    program_parameters = ProgramParameters()\r\n",
    "    num_iterations   = program_parameters['num_iterations']\r\n",
    "    batch_size       = program_parameters['batch_size']\r\n",
    "    corrupt_size     = program_parameters['corrupt_size']\r\n",
    "    batch_iterations = program_parameters['batch_iterations']\r\n",
    "\r\n",
    "    entity_dictionary, num_entities    = Dictionary('datasets/entities.txt')\r\n",
    "    relation_dictionary, num_relations = Dictionary('datasets/relations.txt')\r\n",
    "\r\n",
    "    training_data, num_examples = TrainingData('datasets/train_triplet.txt', entity_dictionary, relation_dictionary)\r\n",
    "\r\n",
    "    word_indices, num_words = WordIndices('datasets/word_indices.pickle')\r\n",
    "    program_parameters['num_entities']  = num_entities\r\n",
    "    program_parameters['num_relations'] = num_relations\r\n",
    "    program_parameters['num_examples']  = num_examples\r\n",
    "    program_parameters['num_words']     = num_words\r\n",
    "    program_parameters['word_indices']  = word_indices\r\n",
    "\r\n",
    "    network = NeuralTensorNetwork(program_parameters)\r\n",
    "\r\n",
    "    for i in range(num_iterations):\r\n",
    "\r\n",
    "        batch_indices = np.random.randint(num_examples, size = batch_size)\r\n",
    "        data          = {}\r\n",
    "        data['rel']   = np.tile(training_data[batch_indices, 1], (1, corrupt_size)).T\r\n",
    "        data['e1']    = np.tile(training_data[batch_indices, 0], (1, corrupt_size)).T\r\n",
    "        data['e2']    = np.tile(training_data[batch_indices, 2], (1, corrupt_size)).T\r\n",
    "        data['e3']    = np.random.randint(num_entities, size = (batch_size * corrupt_size, 1))\r\n",
    "\r\n",
    "        if np.random.random() < 0.5:\r\n",
    "\r\n",
    "            opt_solution = scipy.optimize.minimize(network.costF, network.theta,\r\n",
    "                args = (data, 0,), method = 'L-BFGS-B', jac = True, options = {'maxiter': batch_iterations})\r\n",
    "        else:\r\n",
    "\r\n",
    "            opt_solution = scipy.optimize.minimize(network.costF, network.theta,\r\n",
    "                args = (data, 1,), method = 'L-BFGS-B', jac = True, options = {'maxiter': batch_iterations})\r\n",
    "\r\n",
    "        network.theta = opt_solution.x\r\n",
    "        print(network.theta)\r\n",
    "    dev_data, dev_labels   = getTest('dev.txt', entity_dictionary, relation_dictionary)\r\n",
    "    test_data, test_labels = getTest('test.txt', entity_dictionary, relation_dictionary)\r\n",
    "\r\n",
    "    network.MaxThresholds(dev_data, dev_labels)\r\n",
    "    predictions = network.getPrediction(test_data)\r\n",
    "\r\n",
    "    print(\"Accuracy:\", np.mean((predictions == test_labels)))\r\n",
    "\r\n",
    "NTN()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}